import express from "express";
import { createClient } from "@supabase/supabase-js";
import fetch from "node-fetch";

const router = express.Router();

// Ollama Configuration (FREE local/self-hosted LLM)
const OLLAMA_CONFIG = {
  // For local development
  LOCAL_URL: "http://localhost:11434",

  // For production - use ngrok or Railway.app URL
  PRODUCTION_URL: process.env.OLLAMA_URL || "http://localhost:11434",

  // Model to use (download with: ollama pull mistral)
  MODEL: process.env.OLLAMA_MODEL || "mistral", // or 'llama2', 'phi', 'neural-chat'

  // Use OpenAI as fallback if Ollama fails
  USE_OPENAI_FALLBACK: true,
};

// Initialize Supabase
const supabase = createClient(
  process.env.SUPABASE_URL || "",
  process.env.SUPABASE_SERVICE_KEY || ""
);

// Generate report using Ollama (FREE)
router.post("/generate-report-free", async (req, res) => {
  try {
    const { patientId, physicianPreferences } = req.body;

    // Fetch consultation data
    const { data: consultation, error } = await supabase
      .from("consultations")
      .select("*")
      .eq("patient_id", patientId)
      .single();

    if (error || !consultation) {
      return res.status(404).json({ error: "Consultation not found" });
    }

    // Build prompt
    const prompt = buildMedicalPrompt(consultation, physicianPreferences);

    // Try Ollama first (FREE)
    let htmlReport = "";
    try {
      htmlReport = await generateWithOllama(prompt, physicianPreferences);
    } catch (ollamaError) {
      console.error("Ollama failed:", ollamaError);

      if (OLLAMA_CONFIG.USE_OPENAI_FALLBACK) {
        // Fallback to OpenAI if Ollama fails
        htmlReport = await generateWithOpenAI(prompt, physicianPreferences);
      } else {
        throw new Error("Ollama not available. Please ensure Ollama is running.");
      }
    }

    // Store the generated report
    await supabase.from("ai_processing_logs").insert({
      consultation_id: consultation.id,
      command: "generate_report_ollama",
      input_data: consultation,
      output_data: { html: htmlReport },
      processing_time_ms: Date.now(),
    });

    res.json({
      success: true,
      html: htmlReport,
      consultation: consultation,
      aiProvider: htmlReport.includes("<!-- Generated by Ollama -->") ? "ollama" : "openai",
    });
  } catch (error) {
    console.error("Error generating report:", error);
    res.status(500).json({ error: error.message });
  }
});

// Generate using Ollama (FREE local LLM)
async function generateWithOllama(prompt: string, preferences: any): Promise<string> {
  const ollamaUrl =
    process.env.NODE_ENV === "production" ? OLLAMA_CONFIG.PRODUCTION_URL : OLLAMA_CONFIG.LOCAL_URL;

  const systemPrompt = buildOllamaSystemPrompt(preferences);

  const response = await fetch(`${ollamaUrl}/api/generate`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: OLLAMA_CONFIG.MODEL,
      prompt: `${systemPrompt}\n\nPatient Data:\n${prompt}\n\nGenerate a complete HTML medical report.`,
      stream: false,
      options: {
        temperature: 0.3,
        top_p: 0.9,
        num_predict: 4096,
      },
    }),
  });

  if (!response.ok) {
    throw new Error(`Ollama API error: ${response.statusText}`);
  }

  const result = await response.json();

  // Clean and validate HTML
  let html = result.response;

  // Ensure it's valid HTML
  if (!html.includes("<!DOCTYPE html>")) {
    html = wrapInHTML(html, preferences);
  }

  // Add Ollama marker
  html = html.replace("</body>", "<!-- Generated by Ollama -->\n</body>");

  return html;
}

// Build Ollama-specific prompt
function buildOllamaSystemPrompt(preferences: any): string {
  const lang = preferences?.language || "fr";

  return `You are a medical AI assistant. Generate a complete HTML medical report.

Requirements:
1. Output must be valid HTML starting with <!DOCTYPE html>
2. Use ${lang === "fr" ? "French" : "English"} language
3. Create natural, professional medical language
4. Include these sections: ${getEnabledSections(preferences).join(", ")}
5. Format: "Age ${lang === "fr" ? "ans" : "years"} · Gender · DiagnosisName Percentage%"

Template structure:
- Clinical Strategy with differential diagnosis
- HPI confirmation summary  
- SOAP note (S, O, A, P)
- Follow-up questions (10)
- Medications by diagnosis
- Laboratory tests
- Imaging requisitions
- Specialist referrals
- Work leave declaration
- Insurance declaration

Transform all patient input into professional medical language.`;
}

// Get enabled sections based on preferences
function getEnabledSections(preferences: any): string[] {
  const sections = [];
  if (preferences?.showClinicalStrategy !== false) sections.push("Clinical Strategy");
  if (preferences?.showHPI !== false) sections.push("HPI");
  if (preferences?.showSOAP !== false) sections.push("SOAP");
  if (preferences?.showFollowUpQuestions !== false) sections.push("Follow-up Questions");
  if (preferences?.showMedications !== false) sections.push("Medications");
  if (preferences?.showLaboratory !== false) sections.push("Laboratory");
  if (preferences?.showImaging !== false) sections.push("Imaging");
  if (preferences?.showReferrals !== false) sections.push("Referrals");
  if (preferences?.showWorkLeave !== false) sections.push("Work Leave");
  if (preferences?.showInsuranceDeclaration !== false) sections.push("Insurance");
  return sections;
}

// Wrap content in proper HTML if needed
function wrapInHTML(content: string, preferences: any): string {
  const lang = preferences?.language || "fr";

  return `<!DOCTYPE html>
<html lang="${lang}">
<head>
    <meta charset="UTF-8">
    <title>InstantHPI Medical Report</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1, h2, h3 { color: #2c3e50; }
        table { width: 100%; border-collapse: collapse; margin: 10px 0; }
        td { padding: 8px; border: 1px solid #ddd; }
        .warning { background-color: #f8f9fa; border: 1px solid #dee2e6; padding: 15px; border-radius: 5px; }
    </style>
</head>
<body>
    ${content}
</body>
</html>`;
}

// Build medical prompt from consultation
function buildMedicalPrompt(consultation: any, preferences: any): string {
  const formData = consultation.form_data || {};

  return `
Patient ID: ${consultation.patient_id}
Gender: ${formData.gender}
Age: ${formData.age}
Chief Complaint: ${consultation.chief_complaint}
Symptoms: ${consultation.symptoms}
Duration: ${consultation.duration}
Severity: ${consultation.severity}/10
Location: ${consultation.location}
Trigger: ${consultation.trigger}
Aggravating Factors: ${consultation.aggravating_factors}
Relieving Factors: ${consultation.relieving_factors}
Evolution: ${consultation.evolution}
Associated Symptoms: ${consultation.associated_symptoms}
Treatments Tried: ${consultation.treatments_tried}
Treatment Response: ${consultation.treatment_response}
Chronic Conditions: ${consultation.chronic_conditions}
Allergies: ${consultation.allergies}
Pregnancy/Breastfeeding: ${consultation.pregnancy_breastfeeding}
Additional Notes: ${consultation.other_notes}
`;
}

// Fallback to OpenAI if needed
async function generateWithOpenAI(prompt: string, preferences: any): Promise<string> {
  // This would use the OpenAI implementation from clinical-ai.ts
  // Keeping it here as a fallback option
  const OPENAI_KEY = process.env.OPENAI_API_KEY;

  if (!OPENAI_KEY) {
    throw new Error("No AI provider available. Please set up Ollama or provide OpenAI API key.");
  }

  // Implementation would go here (reuse from clinical-ai.ts)
  return "<html><body><h1>OpenAI Fallback</h1></body></html>";
}

// Setup instructions endpoint
router.get("/ollama-setup", (req, res) => {
  res.json({
    instructions: {
      local: {
        step1: "Install Ollama: curl -fsSL https://ollama.ai/install.sh | sh",
        step2: "Pull a model: ollama pull mistral",
        step3: "Start server: ollama serve",
        step4:
          'Test: curl http://localhost:11434/api/generate -d \'{"model":"mistral","prompt":"Hello"}\'',
      },
      production: {
        option1: {
          name: "Ngrok (easiest)",
          steps: [
            "Install ngrok: brew install ngrok",
            "Run: ngrok http 11434",
            "Copy the URL and set OLLAMA_URL environment variable",
          ],
        },
        option2: {
          name: "Railway.app (free tier)",
          steps: ["Create Dockerfile with Ollama", "Deploy to Railway", "Use provided URL"],
        },
        option3: {
          name: "Google Colab",
          steps: [
            "Run Ollama in Colab notebook",
            "Use localtunnel to expose",
            "Free GPU for 4-6 hours",
          ],
        },
      },
      models: {
        recommended: [
          { name: "mistral", size: "4.1GB", command: "ollama pull mistral" },
          { name: "llama2:7b", size: "3.8GB", command: "ollama pull llama2:7b" },
          { name: "phi", size: "1.6GB", command: "ollama pull phi" },
          { name: "medllama2", size: "3.8GB", command: "ollama pull medllama2" },
        ],
      },
    },
  });
});

// Check Ollama status
router.get("/ollama-status", async (req, res) => {
  try {
    const ollamaUrl =
      process.env.NODE_ENV === "production"
        ? OLLAMA_CONFIG.PRODUCTION_URL
        : OLLAMA_CONFIG.LOCAL_URL;

    const response = await fetch(`${ollamaUrl}/api/tags`);

    if (response.ok) {
      const data = await response.json();
      res.json({
        status: "online",
        url: ollamaUrl,
        models: data.models || [],
        currentModel: OLLAMA_CONFIG.MODEL,
      });
    } else {
      res.json({
        status: "offline",
        url: ollamaUrl,
        error: "Ollama server not responding",
      });
    }
  } catch (error) {
    res.json({
      status: "offline",
      error: error.message,
      instructions: "Run: ollama serve",
    });
  }
});

export default router;
